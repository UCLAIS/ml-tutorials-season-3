{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d45b15",
   "metadata": {},
   "source": [
    "# 3. Challenge - REINFORCE\n",
    "\n",
    "![cartpole](https://gym.openai.com/videos/2019-04-06--My9IiAbqha/CartPole-v1/poster.jpg)\n",
    "\n",
    "In this notebook, we're going to build a REINFORCE agent that learns to play a CartPole game. OpenAI (famously funded by Elon Musk) provides a package called \"gym\" that has a variety of environments where we can train our agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4892a98",
   "metadata": {},
   "source": [
    "### Basic commands\n",
    "\n",
    "We are aware that it might be your first interaction with OpenAI Gym library, therefore, we are providing you with some relevant commands you might need to use throughout both parts of the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_commands():\n",
    "    \n",
    "    #Creating cartpole environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    #Visualize actions\n",
    "    env.render()\n",
    "    \n",
    "    #Take action and extracting information\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    #Close window\n",
    "    env.close()\n",
    "    \n",
    "    #Extracting the number of possible actions\n",
    "    env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d60ae",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Let's start building our agent by importing the required libraries and specifying parameters. You may play around with some of the values (gamma, learning rate, etc.) to see the effect it has on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64661c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####------Configuration parameters----------------------################\n",
    "\n",
    "seed = 42\n",
    "# Discount factor for past rewards\n",
    "gamma = ___\n",
    "learning_rate = 0.01\n",
    "max_steps_per_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######------Setting up environment----------------------################\n",
    "\n",
    "#Select Cartpole environment\n",
    "env = ___\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a72a5",
   "metadata": {},
   "source": [
    "### Define a model\n",
    "Let's first build a model. It's a simple neural nets that takes in states as inputs and predicts which policy to follow. And that's it! You're soon going to see how we can train this simple model to predict the best policy in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_hiddens1 = 32\n",
    "num_hiddens2 = 32\n",
    "num_actions = ___\n",
    "\n",
    "# define a model\n",
    "inputs = Input(shape=(num_states,))\n",
    "fc1 = Dense(num_hiddens1, activation='relu')(inputs)\n",
    "fc2 = Dense(num_hiddens2, activation='relu')(fc1)\n",
    "outputs = Dense(num_actions, activation='softmax')(fc2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54fb50",
   "metadata": {},
   "source": [
    "### Define a training process\n",
    "\n",
    "REINFORCE is one of the Policy Gradient algorithms that learns to find an optimal policy that maximises the objective function: \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_\\tau P(\\tau; \\theta)R(\\tau)\n",
    "$$\n",
    "\n",
    "where $d_{\\pi}(s)$ is probability of visiting the state $s$. Since it's nearly impossible to know the exact stationary distribution of states $d_{\\pi}(s)$ especially in the continuous space, we'd like to approximate the objective function. In a nutshell, the approximation represents the expected rewards with a given policy. \n",
    "\n",
    "Its derivative that we will use to update our model is\n",
    "\n",
    "$$\n",
    "\\begin {split}\n",
    "\\nabla J(\\theta) \n",
    "& = \\mathbb{E}_{\\pi} [Q_{\\pi}(s,a) {\\nabla}_{\\theta} \\ln{\\pi_\\theta (a|s)}]\n",
    "\\\\ &= \\mathbb{E}_{\\pi} [G_t {\\nabla}_{\\theta} \\ln{\\pi_\\theta (a|s)}]\n",
    "\\end {split}\n",
    "$$\n",
    "\n",
    "according to the [Policy Gradient Theorem](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#policy-gradient-theorem).\n",
    "\n",
    "To train a REINFORCE agent, we have to follow the following steps:\n",
    "\n",
    "1. Initialize the policy with random values\n",
    "2. Sample states, actions, and rewards from one episode\n",
    "3. Update the policy using the gradient of the objective function $J(\\theta)$\n",
    "4. Repeat 2~3 until the policy converges\n",
    "\n",
    "We just assume that the returns we get from each episode when following the current policy represents the objective function fairly well. By sampling them multiple times through thousands of episodes, we will hopefully update the model in the right direction. It's basically how most of the deep learning models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515dbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "action_probs_history = []  # probabilities of the selected actions for each time step\n",
    "rewards_history = []  # rewards for each time step\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()  # reset the environment \n",
    "    episode_reward = 0\n",
    "    \n",
    "    # tf.GradientTape() lets us compute the gradiets of loss with respect to model weights\n",
    "    with tf.GradientTape() as tape: \n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            env.render()\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, axis=0)\n",
    "            \n",
    "            #Using model to output policy\n",
    "            policy = ___\n",
    "            \n",
    "            action = np.random.choice(num_actions, 1, p=np.squeeze(policy))[0]\n",
    "            action_prob = policy[0, action]  # probability of the action taken\n",
    "            \n",
    "            #Extracting environment parameters after the action\n",
    "            state, reward, done, _ = ___\n",
    "            \n",
    "            # collect samples\n",
    "            rewards_history += reward,  # appends a variable to the list\n",
    "            action_probs_history += action_prob,\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "            \n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # compute the loss\n",
    "        \n",
    "        action_probs_history = tf.convert_to_tensor(action_probs_history)\n",
    "        cross_entropy = -tf.math.log(action_probs_history + 1e-6)\n",
    "        loss = tf.reduce_sum(returns * cross_entropy)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)  # compute the gradients of loss w.r.t model parameters\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # update the parameters accordingly\n",
    "        action_probs_history, rewards_history = [], []  # clear the samples\n",
    "        \n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

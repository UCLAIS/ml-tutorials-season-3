{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8264cb90",
   "metadata": {},
   "source": [
    "# 6. Challenge Solution - A2C\n",
    "\n",
    "So far, we managed to implement the REINFORCE agent version for the Cartpole environment. Now let's try to do the A2C one. As both models have a pretty similar structure and functioning mechanism, this notebook will contain more missing parts.\n",
    "\n",
    "### Theory\n",
    "\n",
    "As it has been mentioned in Actor-Critic methods notebook, A2C algorithm has pretty similar expected return expression - the only difference is that the return function is exchanged by the advantage function.\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta)=\\sum_{t = 0}^T\\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t)A(s_t, a_t)\n",
    "$$\n",
    "$$\n",
    "A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n",
    "$$\n",
    "\n",
    "From the structural perspective, A2C model has two parts - actor and critic. Actor takes state as an input and outputs probability distribution for actions, while critic calculated values for those actions.\n",
    "\n",
    "Your implementation should take the following form:\n",
    "1. Extracting environment information (state, action, etc.)\n",
    "2. Passing state through the model to generate action and critic outputs\n",
    "3. Sample action from the probability distribution\n",
    "4. Calculating rewards\n",
    "5. Comparing rewards after taken trajectory to those calculated at the start of the trajectory to generate loss\n",
    "\n",
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1c12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5048bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukeleeai/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "# Discount value\n",
    "gamma = 0.99\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8bc6a",
   "metadata": {},
   "source": [
    "### Defining model\n",
    "\n",
    "As it has been mentioned, A2C algorithm contains two parts - actor and critic - both of which should be modeled by separate neural networks. In our implementation, the input layer is shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00538002",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_hidden = 128\n",
    "num_inputs = 4\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a24be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d6585",
   "metadata": {},
   "source": [
    "Unfortunately, rendering the game display in Jupyter Notebook requires a complex workaround. If you want to render it, copy the entire code and run it in your favorite IDE like PyCharm and uncomment the line `env.render()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08aedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 6.51 at episode 10\n",
      "running reward: 9.25 at episode 20\n",
      "running reward: 10.83 at episode 30\n",
      "running reward: 11.90 at episode 40\n",
      "running reward: 14.54 at episode 50\n",
      "running reward: 18.99 at episode 60\n",
      "running reward: 22.05 at episode 70\n",
      "running reward: 19.99 at episode 80\n",
      "running reward: 19.64 at episode 90\n",
      "running reward: 17.03 at episode 100\n",
      "running reward: 16.06 at episode 110\n",
      "running reward: 16.20 at episode 120\n",
      "running reward: 15.24 at episode 130\n",
      "running reward: 14.72 at episode 140\n",
      "running reward: 16.43 at episode 150\n",
      "running reward: 20.27 at episode 160\n",
      "running reward: 28.29 at episode 170\n",
      "running reward: 36.02 at episode 180\n",
      "running reward: 32.30 at episode 190\n",
      "running reward: 46.09 at episode 200\n",
      "running reward: 55.34 at episode 210\n",
      "running reward: 56.83 at episode 220\n",
      "running reward: 44.71 at episode 230\n",
      "running reward: 43.34 at episode 240\n"
     ]
    }
   ],
   "source": [
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render()  # Uncomment this line to render the display (doesn't work for Jupyter Notebook)\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

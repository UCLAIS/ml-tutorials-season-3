{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d45b15",
   "metadata": {},
   "source": [
    "# 5. Challenge Solution - REINFORCE\n",
    "\n",
    "![cartpole](https://miro.medium.com/max/1200/1*v8KcdjfVGf39yvTpXDTCGQ.gif)\n",
    "\n",
    "In this notebook, we're going to build a REINFORCE agent that learns to play a CartPole game. OpenAI (famously funded by Elon Musk) provides a package called \"gym\" that has a variety of environments where we can train our agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4892a98",
   "metadata": {},
   "source": [
    "### Basic commands\n",
    "\n",
    "We are aware that it might be your first interaction with OpenAI Gym library, therefore, we are providing you with some relevant commands you might need to use throughout both parts of the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f4917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_commands():\n",
    "    \n",
    "    #Creating cartpole environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    #Visualize actions\n",
    "    env.render()\n",
    "    \n",
    "    #Take action and extracting information\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    #Close window\n",
    "    env.close()\n",
    "    \n",
    "    #Extracting the number of possible actions\n",
    "    env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d60ae",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Let's start building our agent by importing the required libraries and specifying parameters. You may play around with some of the values (gamma, learning rate, etc.) to see the effect it has on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64661c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab3bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####------Configuration parameters----------------------################\n",
    "\n",
    "seed = 42\n",
    "# Discount factor for past rewards\n",
    "gamma = 0.99\n",
    "learning_rate = 0.01\n",
    "max_steps_per_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c7841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukeleeai/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "#######------Setting up environment----------------------################\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a72a5",
   "metadata": {},
   "source": [
    "### Define a model\n",
    "Let's first build a model. It's a simple neural nets that takes in states as inputs and predicts which policy to follow. And that's it! You're soon going to see how we can train this simple model to predict the best policy in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5f4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_hiddens1 = 32\n",
    "num_hiddens2 = 32\n",
    "num_actions = env.action_space.n  # there are two actions in this game: left and right\n",
    "\n",
    "# define a model\n",
    "inputs = Input(shape=(num_states,))\n",
    "fc1 = Dense(num_hiddens1, activation='relu')(inputs)\n",
    "fc2 = Dense(num_hiddens2, activation='relu')(fc1)\n",
    "outputs = Dense(num_actions, activation='softmax')(fc2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54fb50",
   "metadata": {},
   "source": [
    "### Define a training process\n",
    "\n",
    "REINFORCE is one of the Policy Gradient algorithms that learns to find an optimal policy that maximises the objective function: \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_\\tau P(\\tau; \\theta)R(\\tau)\n",
    "$$\n",
    "\n",
    "where $d_{\\pi}(s)$ is probability of visiting the state $s$. Since it's nearly impossible to know the exact stationary distribution of states $d_{\\pi}(s)$ especially in the continuous space, we'd like to approximate the objective function. In a nutshell, the approximation represents the expected rewards with a given policy. \n",
    "\n",
    "Its derivative that we will use to update our model is\n",
    "\n",
    "$$\n",
    "\\begin {split}\n",
    "\\nabla J(\\theta) \n",
    "& = \\mathbb{E}_{\\pi} [Q_{\\pi}(s,a) {\\nabla}_{\\theta} \\ln{\\pi_\\theta (a|s)}]\n",
    "\\\\ &= \\mathbb{E}_{\\pi} [G_t {\\nabla}_{\\theta} \\ln{\\pi_\\theta (a|s)}]\n",
    "\\end {split}\n",
    "$$\n",
    "\n",
    "according to the [Policy Gradient Theorem](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#policy-gradient-theorem).\n",
    "\n",
    "To train a REINFORCE agent, we have to follow the following steps:\n",
    "\n",
    "1. Initialize the policy with random values\n",
    "2. Sample states, actions, and rewards from one episode\n",
    "3. Update the policy using the gradient of the objective function $J(\\theta)$\n",
    "4. Repeat 2~3 until the policy converges\n",
    "\n",
    "We just assume that the returns we get from each episode when following the current policy represents the objective function fairly well. By sampling them multiple times through thousands of episodes, we will hopefully update the model in the right direction. It's basically how most of the deep learning models work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e66333",
   "metadata": {},
   "source": [
    "Unfortunately, rendering the game display in Jupyter Notebook requires a complex workaround. If you want to render it, copy the entire code and run it in your favorite IDE like PyCharm and uncomment the line `env.render()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515dbac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 11.74 at episode 10\n",
      "running reward: 23.98 at episode 20\n",
      "running reward: 28.89 at episode 30\n",
      "running reward: 32.15 at episode 40\n",
      "running reward: 37.01 at episode 50\n",
      "running reward: 42.13 at episode 60\n",
      "running reward: 55.84 at episode 70\n",
      "running reward: 87.93 at episode 80\n",
      "running reward: 102.97 at episode 90\n",
      "running reward: 133.17 at episode 100\n",
      "running reward: 154.64 at episode 110\n",
      "running reward: 172.84 at episode 120\n",
      "running reward: 183.74 at episode 130\n",
      "running reward: 187.33 at episode 140\n",
      "running reward: 179.81 at episode 150\n",
      "running reward: 186.53 at episode 160\n",
      "running reward: 191.94 at episode 170\n",
      "running reward: 195.17 at episode 180\n",
      "Solved at episode 180!\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "action_probs_history = []  # probabilities of the selected actions for each time step\n",
    "rewards_history = []  # rewards for each time step\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()  # reset the environment \n",
    "    episode_reward = 0\n",
    "    \n",
    "    # tf.GradientTape() lets us compute the gradiets of loss with respect to model weights\n",
    "    with tf.GradientTape() as tape: \n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render()  # Uncomment this line to render the display (doesn't work for Jupyter Notebook)\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, axis=0)\n",
    "            \n",
    "            policy = model(state)\n",
    "            \n",
    "            action = np.random.choice(num_actions, 1, p=np.squeeze(policy))[0]\n",
    "            action_prob = policy[0, action]  # probability of the action taken\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # collect samples\n",
    "            rewards_history += reward,  # appends a variable to the list\n",
    "            action_probs_history += action_prob,\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "            \n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # compute the loss\n",
    "        \n",
    "        action_probs_history = tf.convert_to_tensor(action_probs_history)\n",
    "        cross_entropy = -tf.math.log(action_probs_history + 1e-6)\n",
    "        loss = tf.reduce_sum(returns * cross_entropy)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)  # compute the gradients of loss w.r.t model parameters\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # update the parameters accordingly\n",
    "        action_probs_history, rewards_history = [], []  # clear the samples\n",
    "        \n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
